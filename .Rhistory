model1<- lm(mpg ~ wt + qsec + am, data=mtcars)
par(mfrow=c(2, 2), plot(model1))
rm(list=ls(all=TRUE))
ls()
detach(lattice)
detach("lattice")
getwd()
ar(mfrow=c(2, 2), plot(model1))
model1<- lm(mpg ~ wt + qsec + am, data=mtcars)
par(mfrow=c(2, 2), plot(model1))
plot(cars)
par(mfrow=c(2,2))
plot(model1)
install.packages("framed")
sudo tlmgr update --self
sudo tlmgr install framed
system('kpsewhich framed.sty')
sessionInfo()
system('kpsewhich framed.sty')
install.packages("texlive-framed")
devtools::install_github("rstudio/rmarkdown")
setwd(/Users/Floret/Desktop/courseraR/machine learning/project)
setwd(Users/Floret/Desktop/courseraR/machine learning/project)
setwd("Users/Floret/Desktop/courseraR/machine learning/project"")
setwd("Users/Floret/Desktop/courseraR/machine learning/project)
setwd("Users/Floret/Desktop/courseraR/machine learning/project")
setwd("~/Desktop/courseraR/machine learning/project")
train<- read.csv("pml-training.csv")
test<- read.csv("pml-training.csv")
View(test)
View(train)
View(test)
View(test)
View(test)
View(test)
View(test)
View(train)
View(train)
setwd("~/Desktop/courseraR/machine learning/project")
train<- read.csv("pml-training.csv", na.strings = c("NA", ""))
test<- read.csv("pml-training.csv",na.strings = c("NA", ""))
setwd("~/Desktop/courseraR/machine learning/project")
train<- read.csv("pml-training.csv", na.strings = c("NA", ""))
test<- read.csv("pml-training.csv",na.strings = c("NA", ""))
library(caret)
inTrain<-createDataPartition(y=train$classe, p=0.6,list=F)
training<-train[inTrain,]
validate<-train[-inTrain,]
rainPar <- trainControl(allowParallel = TRUE, method = "cv", number = 5);
modelFit <- train(classe ~ ., data = training, method="rf",
trainControl = trainPar, importance=TRUE);
trainPar <- trainControl(allowParallel = TRUE, method = "cv", number = 5);
modelFit <- train(classe ~ ., data = training, method="rf",
trainControl = trainPar, importance=TRUE);
rm(list=ls(all=TRUE))
library(caret)
inTrain<-createDataPartition(y=train$classe, p=0.6,list=F)
training<-train[inTrain,]
validate<-train[-inTrain,]
train<- read.csv("pml-training.csv", na.strings = c("NA", ""))
test<- read.csv("pml-training.csv",na.strings = c("NA", ""))
library(caret)
inTrain<-createDataPartition(y=train$classe, p=0.6,list=F)
training<-train[inTrain,]
validate<-train[-inTrain,]
trainPar <- trainControl(allowParallel = TRUE, method = "cv", number = 5);
modelFit <- train(classe ~ ., data = training, method="rf",
trainControl = trainPar, importance=TRUE);
train<- read.csv("pml-training.csv", na.strings = c("NA", ""))
test<- read.csv("pml-training.csv",na.strings = c("NA", ""))
trainPar <- trainControl(allowParallel = TRUE, method = "cv", number = 5);
modelFit <- train(classe ~ ., data = training, method="rf",
trainControl = trainPar, importance=TRUE);
View(test)
train<- read.csv("pml-training.csv", na.strings = c("NA", "", "#DIV/0!"))
test<- read.csv("pml-training.csv",na.strings = c("NA", "", "#DIV/0!"))
library(caret)
inTrain<-createDataPartition(y=train$classe, p=0.6,list=F)
training<-train[inTrain,]
validate<-train[-inTrain,]
trainPar <- trainControl(allowParallel = TRUE, method = "cv", number = 5);
modelFit <- train(classe ~ ., data = training, method="rf",
trainControl = trainPar, importance=TRUE);
View(train)
test$classe <- 1:nrow(test);
set.seed(0000)
accuracy_rf <- numeric(5)
for (j in 1:5) {
# Splits into 10 folds.
folds <- createFolds(y = training$classe, k = 10, list = FALSE)
correct <- logical(nrow(training))
for (i in 1:10) {
# Split into training and validation sets
# =======================================
train_subset <- training[folds != i, ]
test_subset  <- training[folds == i, ]
# Pre-processing
# ==============
# Remove features with missing values
missing <- is.na(train_subset)
keep_columns <- names(which(colSums(missing) == 0))
train_subset <- train_subset[, keep_columns]
test_subset  <- test_subset[, keep_columns]
# Fit the model
# =============
model <- randomForest(classe ~ ., data = train_subset)
# Record which guesses are correct
# ================================
predictions <- predict(model, newdata = test_subset)
correct[folds == i] <- (predictions == test_subset$classe)
}
# Accuracy for the j-th iteration
accuracy_rf[j] <- mean(correct)
}
library(e1071)
library(randomForest)
set.seed(0000)
accuracy_rf <- numeric(5)
for (j in 1:5) {
# Splits into 10 folds.
folds <- createFolds(y = training$classe, k = 10, list = FALSE)
correct <- logical(nrow(training))
for (i in 1:10) {
# Split into training and validation sets
# =======================================
train_subset <- training[folds != i, ]
test_subset  <- training[folds == i, ]
# Pre-processing
# ==============
# Remove features with missing values
missing <- is.na(train_subset)
keep_columns <- names(which(colSums(missing) == 0))
train_subset <- train_subset[, keep_columns]
test_subset  <- test_subset[, keep_columns]
# Fit the model
# =============
model <- randomForest(classe ~ ., data = train_subset)
# Record which guesses are correct
# ================================
predictions <- predict(model, newdata = test_subset)
correct[folds == i] <- (predictions == test_subset$classe)
}
# Accuracy for the j-th iteration
accuracy_rf[j] <- mean(correct)
}
install.packages("e1071")
install.packages("randomForest")
library(e1071)
library(randomForest)
set.seed(0000)
accuracy_rf <- numeric(5)
for (j in 1:5) {
# Splits into 10 folds.
folds <- createFolds(y = training$classe, k = 10, list = FALSE)
correct <- logical(nrow(training))
for (i in 1:10) {
# Split into training and validation sets
# =======================================
train_subset <- training[folds != i, ]
test_subset  <- training[folds == i, ]
# Pre-processing
# ==============
# Remove features with missing values
missing <- is.na(train_subset)
keep_columns <- names(which(colSums(missing) == 0))
train_subset <- train_subset[, keep_columns]
test_subset  <- test_subset[, keep_columns]
# Fit the model
# =============
model <- randomForest(classe ~ ., data = train_subset)
# Record which guesses are correct
# ================================
predictions <- predict(model, newdata = test_subset)
correct[folds == i] <- (predictions == test_subset$classe)
}
# Accuracy for the j-th iteration
accuracy_rf[j] <- mean(correct)
}
accuracy_rf[j] <- mean(correct)
library(ggplot2)
library(scales)
library(dplyr)
install.packages("dplyr")
library(e1071)
library(randomForest)
library(ggplot2)
library(scales)
library(dplyr)
set.seed(0000)
accuracy_rf <- numeric(5)
for (j in 1:5) {
# Splits into 10 folds.
folds <- createFolds(y = training$classe, k = 10, list = FALSE)
correct <- logical(nrow(training))
for (i in 1:10) {
# Split into training and validation sets
# =======================================
train_subset <- training[folds != i, ]
test_subset  <- training[folds == i, ]
# Pre-processing
# ==============
# Remove features with missing values
missing <- is.na(train_subset)
keep_columns <- names(which(colSums(missing) == 0))
train_subset <- train_subset[, keep_columns]
test_subset  <- test_subset[, keep_columns]
# Fit the model
# =============
model <- randomForest(classe ~ ., data = train_subset)
# Record which guesses are correct
# ================================
predictions <- predict(model, newdata = test_subset)
correct[folds == i] <- (predictions == test_subset$classe)
}
# Accuracy for the j-th iteration
accuracy_rf[j] <- mean(correct)
}
library(caret)
inTrain<-createDataPartition(y=train$classe, p=0.6,list=F)
training<-train[inTrain,]
validate<-train[-inTrain,]
library(e1071)
library(randomForest)
library(ggplot2)
library(scales)
library(dplyr)
set.seed(0000)
accuracy_rf <- numeric(5)
for (j in 1:5) {
# Splits into 10 folds.
folds <- createFolds(y = training$classe, k = 10, list = FALSE)
correct <- logical(nrow(training))
for (i in 1:10) {
# Split into training and validation sets
# =======================================
train_subset <- training[folds != i, ]
test_subset  <- training[folds == i, ]
# Pre-processing
# ==============
# Remove features with missing values
missing <- is.na(train_subset)
keep_columns <- names(which(colSums(missing) == 0))
train_subset <- train_subset[, keep_columns]
test_subset  <- test_subset[, keep_columns]
# Fit the model
# =============
model <- randomForest(classe ~ ., data = train_subset)
# Record which guesses are correct
# ================================
predictions <- predict(model, newdata = test_subset)
correct[folds == i] <- (predictions == test_subset$classe)
}
# Accuracy for the j-th iteration
accuracy_rf[j] <- mean(correct)
}
rm(list=ls(all=TRUE))
setwd(~/Floret/Desktop/courseraR/machine learning)
setwd(~Floret/Desktop/courseraR/machine learning)
setwd(~/Desktop/courseraR/machine learning)
setwd(~/Desktop/courseraR/machine learning)
setwd("~/Desktop/courseraR/machine learning")
train<- read.csv("pml-training.csv", na.strings = c("NA", "", "#DIV/0!"))
setwd("~/Desktop/courseraR/machine learning/project")
train<- read.csv("pml-training.csv", na.strings = c("NA", "", "#DIV/0!"))
test<- read.csv("pml-training.csv",na.strings = c("NA", "", "#DIV/0!"))
library(caret)
inTrain<-createDataPartition(y=train$class, p=0.6,list=F)
training<-train[inTrain,]
validate<-train[-inTrain,]
library(e1071)
library(randomForest)
library(ggplot2)
library(scales)
library(dplyr)
set.seed(0000)
accuracy_rf <- numeric(5)
View(train)
accuracy_rf <- numeric(5)
for (j in 1:5) {
# Splits into 10 folds.
folds <- createFolds(y = training$class, k = 10, list = FALSE)
correct <- logical(nrow(training))
for (i in 1:10) {
# Split into training and validation sets
# =======================================
train_subset <- training[folds != i, ]
test_subset  <- training[folds == i, ]
# Pre-processing
# ==============
# Remove features with missing values
missing <- is.na(train_subset)
keep_columns <- names(which(colSums(missing) == 0))
train_subset <- train_subset[, keep_columns]
test_subset  <- test_subset[, keep_columns]
# Fit the model
# =============
model <- randomForest(classe ~ ., data = train_subset)
# Record which guesses are correct
# ================================
predictions <- predict(model, newdata = test_subset)
correct[folds == i] <- (predictions == test_subset$classe)
}
accuracy_rf[j] <- mean(correct)
}
# remove if na's appear on more than 90% of total cases
lotOfNAs <- function(vector){
if(sum(is.na(vector))/length(vector) > 0.9){ # if vector is made of more than 90% NAs
res <- TRUE;                             # return true
}else{                                       # if it doesn't
res <- FALSE;                            # return false
}
invisible(res);                              # return the answer
}
##################### function 2: 'preProcessDataFrame'
# function that receive a dataframe and perform its preprocessing
preProcessDataFrame <- function(dataFrame){
subsetTraining <- dataFrame[,-(1:7)]; # manually remove non significant values
end <- ncol(subsetTraining)           # get end (class) index
# convert everything but the class into numeric
subsetTraining[,-end] <- data.frame(sapply(subsetTraining[,-end],as.numeric))
# verify which columns are made most of NAs
varsWith90NAs <- sapply(subsetTraining, lotOfNAs);
# remove these columns
subsetTraining <- subsetTraining[,!varsWith90NAs];
# detect variables who don't contribute for the classification
nzv <- nearZeroVar(subsetTraining[,-end],saveMetrics = TRUE)
subsetTraining <- subsetTraining[,!as.logical(nzv$nzv)]
if(any(is.na(subsetTraining))){               # if there are any remaining NA's
# imput these missing values
preProc <- preProcess(subsetTraining[,-end],method="bagImpute")
subsetTraining[,-end] <- predict(preProc,subsetTraining[,-end])
remove("preProc")                         # memory release
}
invisible(subsetTraining);
}
library(caret)                              # import caret package
## Loading required package: lattice
## Loading required package: ggplot2
set.seed(1234)                              # set random number generation seed
# read training data
setwd("~/Desktop/courseraR/machine learning/project")
training<- read.csv("pml-training.csv", na.strings = c("NA", "", "#DIV/0!"))
#split into training and validation
subsetTrainingIndex <- createDataPartition(training$classe, p=0.6, list = FALSE);
subsetTraining <- training[subsetTrainingIndex,];
# preprocess dataframe
subsetTraining <- preProcessDataFrame(subsetTraining);
# remove if na's appear on more than 90% of total cases
lotOfNAs <- function(vector){
if(sum(is.na(vector))/length(vector) > 0.9){ # if vector is made of more than 90% NAs
res <- TRUE;                             # return true
}else{                                       # if it doesn't
res <- FALSE;                            # return false
}
invisible(res);                              # return the answer
}
##################### function 2: 'preProcessDataFrame'
# function that receive a dataframe and perform its preprocessing
preProcessDataFrame <- function(dataFrame){
subsetTraining <- dataFrame[,-(1:7)]; # manually remove non significant values
end <- ncol(subsetTraining)           # get end (class) index
# convert everything but the class into numeric
subsetTraining[,-end] <- data.frame(sapply(subsetTraining[,-end],as.numeric))
# verify which columns are made most of NAs
varsWith90NAs <- sapply(subsetTraining, lotOfNAs);
# remove these columns
subsetTraining <- subsetTraining[,!varsWith90NAs];
# detect variables who don't contribute for the classification
nzv <- nearZeroVar(subsetTraining[,-end],saveMetrics = TRUE)
subsetTraining <- subsetTraining[,!as.logical(nzv$nzv)]
if(any(is.na(subsetTraining))){               # if there are any remaining NA's
# imput these missing values
preProc <- preProcess(subsetTraining[,-end],method="bagImpute")
subsetTraining[,-end] <- predict(preProc,subsetTraining[,-end])
remove("preProc")                         # memory release
}
invisible(subsetTraining);
}
library(caret)                              # import caret package
## Loading required package: lattice
## Loading required package: ggplot2
set.seed(1234)                              # set random number generation seed
# read training data
setwd("~/Desktop/courseraR/machine learning/project")
training<- read.csv("pml-training.csv", na.strings = c("NA", "", "#DIV/0!"))
#split into training and validation
subsetTrainingIndex <- createDataPartition(training$classe, p=0.6, list = FALSE);
subsetTraining <- training[subsetTrainingIndex,];
# preprocess dataframe
subsetTraining <- preProcessDataFrame(subsetTraining);
# model fit using random forests
trainPar <- trainControl(allowParallel = TRUE, method = "cv", number = 5);
modelFit <- train(classe ~ ., data = subsetTraining, method="rf",
trainControl = trainPar, importance=TRUE);
rm(list=ls(all=TRUE))
setwd("~/Desktop/courseraR/machine learning/project")
train<- read.csv("pml-training.csv", na.strings = c("NA", "", "#DIV/0!"))
test<- read.csv("pml-training.csv",na.strings = c("NA", "", "#DIV/0!"))
View(train)
library(caret)
inTrain<-createDataPartition(y=train$class, p=0.6,list=F)
training<-train[inTrain,]
validate<-train[-inTrain,]
View(training)
View(training)
View(inTrain)
View(inTrain)
View(test)
View(test)
modelFit<- train(classe~ ., data=training, method="rf", prox=TRUE)
isAnyMissing <- sapply(test, function (x) any(is.na(x) | x == ""))
isPredictor <- !isAnyMissing & grepl("belt|[^(fore)]arm|dumbbell|forearm", names(isAnyMissing))
predCandidates <- names(isAnyMissing)[isPredictor]
predCandidates
varToInclude <- c("classe", predCandidates)
training <- training[, varToInclude, with=FALSE]
dim(training)
library(caret)
rm(list=ls(all=TRUE))
lotOfNAs <- function(vector){
if(sum(is.na(vector))/length(vector) > 0.9){ # if vector is made of more than 90% NAs
res <- TRUE;                             # return true
}else{                                       # if it doesn't
res <- FALSE;                            # return false
}
invisible(res);                              # return the answer
}
preProcessDataFrame <- function(dataFrame){
subsetTraining <- dataFrame[,-(1:7)]; # manually remove non significant values
end <- ncol(subsetTraining)           # get end (class) index
# convert everything but the class into numeric
subsetTraining[,-end] <- data.frame(sapply(subsetTraining[,-end],as.numeric))
# verify which columns are made most of NAs
varsWith90NAs <- sapply(subsetTraining, lotOfNAs);
# remove these columns
subsetTraining <- subsetTraining[,!varsWith90NAs];
# detect variables who don't contribute for the classification
nzv <- nearZeroVar(subsetTraining[,-end],saveMetrics = TRUE)
subsetTraining <- subsetTraining[,!as.logical(nzv$nzv)]
if(any(is.na(subsetTraining))){               # if there are any remaining NA's
# imput these missing values
preProc <- preProcess(subsetTraining[,-end],method="bagImpute")
subsetTraining[,-end] <- predict(preProc,subsetTraining[,-end])
remove("preProc")                         # memory release
}
invisible(subsetTraining);
}
library(caret)
set.seed(1234)                              # set random number generation seed
# read training data
setwd("~/Desktop/courseraR/machine learning/project")
training <- read.csv("pml-training.csv");
#split into training and validation
subsetTrainingIndex <- createDataPartition(training$classe, p=0.99, list = FALSE);
subsetTraining <- training[subsetTrainingIndex,];
# preprocess dataframe
subsetTraining <- preProcessDataFrame(subsetTraining);
library(caret)
inTrain<-createDataPartition(y=train$class, p=0.6,list=F)
training<-train[inTrain,]
validate<-train[-inTrain,]
View(subsetTraining)
rm(list=ls(all=TRUE))
# model fit using random forests
trainPar <- trainControl(allowParallel = TRUE, method = "cv", number = 5);
modelFit <- train(classe ~ ., data = subsetTraining, method="rf",
trainControl = trainPar, importance=TRUE);
lotOfNAs <- function(vector){
if(sum(is.na(vector))/length(vector) > 0.9){ # if vector is made of more than 90% NAs
res <- TRUE;                             # return true
}else{                                       # if it doesn't
res <- FALSE;                            # return false
}
invisible(res);                              # return the answer
}
##################### function 2: 'preProcessDataFrame'
# function that receive a dataframe and perform its preprocessing
preProcessDataFrame <- function(dataFrame){
subsetTraining <- dataFrame[,-(1:7)]; # manually remove non significant values
end <- ncol(subsetTraining)           # get end (class) index
# convert everything but the class into numeric
subsetTraining[,-end] <- data.frame(sapply(subsetTraining[,-end],as.numeric))
# verify which columns are made most of NAs
varsWith90NAs <- sapply(subsetTraining, lotOfNAs);
# remove these columns
subsetTraining <- subsetTraining[,!varsWith90NAs];
# detect variables who don't contribute for the classification
nzv <- nearZeroVar(subsetTraining[,-end],saveMetrics = TRUE)
subsetTraining <- subsetTraining[,!as.logical(nzv$nzv)]
if(any(is.na(subsetTraining))){               # if there are any remaining NA's
# imput these missing values
preProc <- preProcess(subsetTraining[,-end],method="bagImpute")
subsetTraining[,-end] <- predict(preProc,subsetTraining[,-end])
remove("preProc")                         # memory release
}
invisible(subsetTraining);
}
library(caret)                              # import caret package
## Loading required package: lattice
## Loading required package: ggplot2
set.seed(1234)                              # set random number generation seed
# read training data
setwd("~/Desktop/courseraR/machine learning/project")
training <- read.csv("pml-training.csv");
#split into training and validation
subsetTrainingIndex <- createDataPartition(training$classe, p=0.99, list = FALSE);
subsetTraining <- training[subsetTrainingIndex,];
# preprocess dataframe
subsetTraining <- preProcessDataFrame(subsetTraining);
trainPar <- trainControl(allowParallel = TRUE, method = "cv", number = 5);
modelFit <- train(classe ~ ., data = subsetTraining, method="rf",
trainControl = trainPar, importance=TRUE);
